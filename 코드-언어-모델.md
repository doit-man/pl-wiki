## 개요

코드 언어 모델은 언어 모델<sup>[1](#참고-문서)</sup> 기술을 이용하여 프로그램 소스 코드를 학습한 모델이다. 프로그램 소스 코드의 자연어적인 특징에 착안해서 소스 코드를 자연어 텍스트처럼 다룬다. 특히 코드를 생성하는 분야에 강하다.

## 프로그램 코드의 양면성

프로그램 코드는 계산을 표기하는 수단이다. 문법 구조 (syntax) 와 의미 구조 (semantics) 가 엄밀하게 정의되어 있는 프로그래밍 언어로 작성되어 있어 기계가 이해하기 쉽다.
논리적인 계산 과정을 표현하기 위하여 사람이 인공적으로 정의한 언어이기 때문에 자연어와는 다른 종류로 취급되었다.

그러나 2012년 ICSE 에서 프로그램 코드 역시 자연어적인 특징을 가진다는 주장이 제기되었다<sup>[2](#참고-문서)</sup>. 프로그램 코드는 사람이 작성하고 사람이 읽기 때문에 자연어와 마찬가지로 통계적인 특징이 있다. 이후로 자연어 처리 분야의 기술을 프로그램 코드에 응용한 다양한 기술이 성과를 내고 있는데, 코드 언어 모델 역시 그 일종이다.

## 모델 종류

#### 통계적 모델: N-gram

(N-1)개의 연속된 토큰이 주어졌을 때 N번째 토큰을 예측하는 통계적 모델이다. 조건부 확률을 이용한다.

```
P(t_1, t_2, ..., t_n) = P(t_n|t_1, t_2, ..., t_(n-1)) * ... * P(t_2|t_1)
```

N-gram 모델에서 N개의 고정된 토큰 길이 내에서 코드의 문맥을 파악해야한다는 제약이 있다. 맥락을 이용하는 자연어에 비해 프로그램 코드는 문맥을 모두 코드에 작성하기 때문에 내용이 길어진다. 하지만 길어진 내용에 변수나 함수 이름이 반복적으로 등장하기 때문에 지역성(locality)이 높아 학습에 유리하다<sup>[2](#참고-문서)</sup>.

N-gram 학습에서 문맥 크기 N의 크기가 커질수록 계산 복잡도는 기하급수적으로 늘어난다. Raychev 등<sup>[3](#참고-문서)</sup>은 코드 자동 완성 모델을 만드는 데 N-gram 을 활용했는데, 이때 사용한 문맥 크기는 3으로 매우 작다. 이후에는 문맥 크기의 제약이 없는 순환 신경망(RNN) 모델이 점차 N-gram 을 대체하게 되었다.

#### 기계 학습 딥러닝 모델: 트랜스포머(Transformer)

트랜스포머<sup>[4](#참고-문서)</sup> 는 2017년 구글이 제안한 딥러닝 모델로 인코더-디코더 구조를 따른다. 자연어 처리 분야에서는 트랜스포머 인코더를 이용한 BERT<sup>[5](#참고-문서)</sup>, 트랜스포머 디코더를 이용한 GPT<sup>[6](#참고-문서)</sup>, 트랜스포머 인코더-디코더를 모두 이용한 T5<sup>[7](#참고-문서)</sup> 등 모델이 연이어 공개되었다. 이들은 자연어를 이해하고 생성하는 분야의 발전을 이끌고 있다.

CodeBERT<sup>[8](#참고-문서)</sup>는 BERT 구조를 이용해서 프로그램 코드를 학습한 모델이다. 프로그램 코드를 자연어와 완전히 동일한 방식으로 단어를 분절(Tokenize)하고 모델에 입력하여 학습한다. 프로그램 코멘트에 포함된 자연어와 프로그램 코드를 연관지어서 학습하는 방법으로 자연어의 의미와 프로그램의 의미를 연결하는 코드 검색(Natual language code search)과 개발 문서 생성(Code documentation generation) 과제에서 성능을 보였다.

CodeT5<sup>[9](#참고-문서)</sup>는 T5의 구조에 프로그램 코드를 학습한 모델인데, 이때 변수 이름에 집중하여 프로그램 코드의 특성을 활용하고자했다. 코드를 단순히 자연어처럼 분절할 뿐만 아니라, 변수명이었던 토큰의 위치와 의미를 학습한다. 결함과 복제 코드(clone)를 탐지하는 데 CodeBERT 보다 우수한 성능을 냈고, 디코더 구조를 활용해서 코드 번역이나 간단한 자동 수정도 가능하다.

트랜스포머는 N-gram 과 마찬가지로 고정된 문맥 길이를 사용하지만 N-gram 에 비해 문맥의 크기가 크다. CodeBERT 와 CodeT5는 한번에 토큰 512개까지 입력받을 수 있다.

#### 초거대 언어 모델(Large Language Model, LLM)

대규모 파라미터에 대규모 데이터를 학습하여 구축된 트랜스포머 언어 모델을 초거대 언어모델이라고 부른다. 이때 파라미터 갯수는 인코더, 디코더의 레이어 갯수, 문맥의 길이, 은닉층의 크기 등 다양한 구조적 변수에 의해 결정된다.

GPT-3<sup>[10](#참고-문서)</sup>는 문맥 길이를 4,096개까지 허용하고 디코더 레이어를 96개 쌓아 1,750억 파라미터 규모로 만들어진 모델이다. 초기 GPT 모델이 1.1억개 파라미터 규모였던 데 비해 1,500배 이상 커졌다. 규모가 커진 만큼 모델을 충분히 학습하기 위해 데이터를 많이 학습해야하는데, GPT-3 는 이 과정에 프로그램 코드를 같이 학습해서 자연어와 코드를 동시에 이해하고 생성할 수 있는 모델이 되었다.

Codex<sup>[11](#참고-문서)</sup>는 GPT-3 의 120억 파라미터 버전에 코드 데이터를 많이 학습시킨 모델이다. 자연어 명령을 보고 코드를 생성하는 데 특화되었다. 이는 단순 번역보다 복잡한 작업인데, 자연어로 작성된 명령문의 의도를 파악하고 요구사항에 맞는 코드를 생성해야하기 때문이다. Codex는 GitHub 에서 제공하는 Copilot 도구에 통합되어서 상용 서비스로 출시되었다.

## 한계

코드 언어 모델은 데이터에서 자주 발견되는 취약점을 학습하고 모방하기 쉽다. CodeBERT 와 CodeT5, Codex 모두 GitHub에 공개된 오픈소스 코드를 학습하였다. 그러나 오픈소스에는 다양한 취약점이 있으며, 이런 취약점이 반복적으로 나타나고 있다<sup>[12](#참고-문서)</sup>. 이런 반복되는 취약점은 모델이 학습해서 따라하기 쉽다.

코드 언어 모델이 프로그램 코드의 문법적인 규칙과 성질을 무시한다는 점도 한계이다. Wang 등<sup>[13](#참고-문서)</sup>의 실험에서 코드 언어 모델이 생성하는 코드의 단 44% 만이 정상적으로 컴파일이 가능했다. Jain 등<sup>[14](#참고-문서)</sup> 역시 GPT-3나 Codex 모델이 생성한 코드 중 30%-60% 만 올바른 코드였다고 지적한다.

코드 언어 모델이 생성하는 코드의 저작권 역시 아직 해결해야할 문제다. Copilot 이 생성하는 코드에 개인적인 정보가 기술되어있거나, 저작권 문구가 그대로 포함되어있는 경우가 발견되면서 비판이 있었다<sup>[15](#참고-문서)</sup>. GitHub의 오픈 소스 학습 방식을 거부한 소프트웨어 자유 재단(Software freedom Conservancy)은 GitHub을 떠나겠다고 선언하기도 했다<sup>[16](#참고-문서)</sup>. 2022 년 11월 3일 현재 GitHub와 Microsoft, OpenAI 를 상대로 Copilot 의 저작권 문제를 지적하는 소송이 제기되었다<sup>[17](#참고-문서)</sup>.

## 참고 문서

[1] 유원준, 안상준. 2021, [“딥 러닝을 이용한 자연어 처리 입문”](https://wikidocs.net/21668)

[2] Hindle, Abram, et al. "On the naturalness of software." *2012 34th International Conference on Software Engineering (ICSE)*. IEEE, 2012.

[3] Raychev, Veselin, Martin Vechev, and Eran Yahav. "Code completion with statistical language models." *Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)*. 2014.

[4] Vaswani, Ashish, et al. "Attention is all you need." *Advances in neural information processing systems* 30 (2017).

[5] Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." *arXiv preprint arXiv:1810.04805* (2018).

[6] Radford, Alec, et al. "Improving language understanding by generative pre-training." (2018).

[7] Raffel, Colin, et al. "Exploring the limits of transfer learning with a unified text-to-text transformer." *J. Mach. Learn. Res.* 21.140 (2020): 1-67.

[8] Feng, Zhangyin, et al. "Codebert: A pre-trained model for programming and natural languages." *arXiv preprint arXiv:2002.08155* (2020).

[9] Wang, Yue, et al. "Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation." *arXiv preprint arXiv:2109.00859* (2021).

[10] Brown, Tom, et al. "Language models are few-shot learners." *Advances in neural information processing systems* 33 (2020): 1877-1901.

[11] Chen, Mark, et al. "Evaluating large language models trained on code." *arXiv preprint arXiv:2107.03374* (2021).

[12] [[반복되는 오류|반복되는-오류]]

[13] Wang, Xin, et al. "Compilable Neural Code Generation with Compiler Feedback." *Findings of the Association for Computational Linguistics: ACL 2022*. 2022.

[14] Jain, Naman, et al. "Jigsaw: Large language models meet program synthesis." *Proceedings of the 44th International Conference on Software Engineering*. 2022.

[15] Matthew Butterick, 25 June 2022, [“This copilot is stupid and wants to kill me”](https://matthewbutterick.com/chron/this-copilot-is-stupid-and-wants-to-kill-me.html)

[16] Software Freedom Conservancy, 29 June 2022, [“Give Up GitHub!”](https://sfconservancy.org/GiveUpGitHub/)

[17] Matthew Butterick, 3 November 2022, [GitHub Copilot litigation · Joseph Saveri Law Firm & Matthew Butterick](https://githubcopilotlitigation.com/)